{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shutil\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, estimate_bandwidth\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'dataset'  \n",
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "df['eye_color_score'] = 0\n",
    "df['skin_color_score'] = 0\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "face_locations = []\n",
    "\n",
    "#function to calculate the color score using HSV color space\n",
    "def calculate_color_score_hsv(region):\n",
    "    hsv_region = cv2.cvtColor(region, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    avg_hsv_per_row = np.mean(hsv_region, axis=0)\n",
    "    avg_hsv = np.mean(avg_hsv_per_row, axis=0)  # [H, S, V]\n",
    "    \n",
    "    hue = avg_hsv[0] * 2  #Scaling back to [0, 360] degrees\n",
    "    saturation = avg_hsv[1] / 255.0  #Normalize to [0, 1]\n",
    "    value = avg_hsv[2] / 255.0  #Normalize to [0, 1]\n",
    "    \n",
    "    if hue < 30 or hue > 330:  #red\n",
    "        color_score = 100 + saturation * 50 + value * 50\n",
    "    elif 30 <= hue < 90:  #yellow-green\n",
    "        color_score = 120 + saturation * 50 + value * 50\n",
    "    elif 90 <= hue < 150:  #green\n",
    "        color_score = 140 + saturation * 50 + value * 50\n",
    "    elif 150 <= hue < 210:  #blue \n",
    "        color_score = 160 + saturation * 50 + value * 50\n",
    "    elif 210 <= hue < 270:  #purple \n",
    "        color_score = 180 + saturation * 50 + value * 50\n",
    "    elif 270 <= hue < 330:  #pink-red\n",
    "        color_score = 190 + saturation * 50 + value * 50\n",
    "    else:\n",
    "        #fallback color score\n",
    "        color_score = 150 + saturation * 50 + value * 50\n",
    "        \n",
    "    return color_score\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    image_id = row['image_id']\n",
    "    image_path = os.path.join(image_folder, f\"{image_id}\")\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        df.loc[index, 'skin_color_score'] = 404\n",
    "        df.loc[index, 'eye_color_score'] = 404\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=7)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        (x, y, w, h) = faces[0]\n",
    "        face_locations.append((x, y, w, h)) \n",
    "    else:\n",
    "        if len(face_locations) > 0:\n",
    "            avg_face_location = np.mean(face_locations, axis=0).astype(int)\n",
    "            (x, y, w, h) = avg_face_location\n",
    "\n",
    "    face_region = image[y:y+h, x:x+w]\n",
    "\n",
    "    #skin color extraction\n",
    "    skin_region = face_region[int(h/4):int(3*h/4), int(w/4):int(3*w/4)]\n",
    "    skin_color_score = calculate_color_score_hsv(skin_region)\n",
    "\n",
    "    #manual Eye color extraction\n",
    "    left_eye_region = face_region[int(h/5):int(h/2.5), int(w/8):int(w/4)]\n",
    "    right_eye_region = face_region[int(h/5):int(h/2.5), int(5*w/8):int(3*w/4)]\n",
    "    \n",
    "    left_eye_color_score = calculate_color_score_hsv(left_eye_region)\n",
    "    right_eye_color_score = calculate_color_score_hsv(right_eye_region)\n",
    "    \n",
    "    eye_color_score = (left_eye_color_score + right_eye_color_score) / 2\n",
    "\n",
    "    df.loc[index, 'skin_color_score'] = skin_color_score\n",
    "    df.loc[index, 'eye_color_score'] = eye_color_score\n",
    "\n",
    "df.to_csv('features.csv', index=False)\n",
    "\n",
    "print(\"Features updated!\")\n",
    "\n",
    "#function to display the image with bounding boxes for face and manual eyes\n",
    "def display_image_with_bounding_boxes(image, face_rect, left_eye_rect, right_eye_rect, opencv_eye_rects):\n",
    "    (x, y, w, h) = face_rect\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)  #blue rectangle for face\n",
    "\n",
    "    #draw manual bounding boxes for the left and right eyes (green)\n",
    "    (lx, ly, lw, lh) = left_eye_rect\n",
    "    cv2.rectangle(image, (lx, ly), (lx + lw, ly + lh), (0, 255, 0), 2)  \n",
    "\n",
    "    (rx, ry, rw, rh) = right_eye_rect\n",
    "    cv2.rectangle(image, (rx, ry), (rx + rw, ry + rh), (0, 255, 0), 2)  \n",
    "    \n",
    "    #draw OpenCV eye bounding boxes (yellow)\n",
    "    for (ex, ey, ew, eh) in opencv_eye_rects:\n",
    "        cv2.rectangle(image, (ex, ey), (ex + ew, ey + eh), (0, 255, 255), 2)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "#process 10 random images to show\n",
    "sample_images = df.sample(10)\n",
    "\n",
    "for index, row in sample_images.iterrows():\n",
    "    image_id = row['image_id']\n",
    "    image_path = os.path.join(image_folder, f\"{image_id}\")\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    #manual Eye Detection\n",
    "    left_eye_rect = (x + int(w / 8), y + int(h / 5), int(w / 4), int(h / 4))\n",
    "    right_eye_rect = (x + int(5 * w / 8), y + int(h / 5), int(w / 4), int(h / 4)) \n",
    "    \n",
    "    #detect eyes using OpenCV's eye detector\n",
    "    face_region_gray = gray[y:y+h, x:x+w]\n",
    "    opencv_eye_rects = eye_cascade.detectMultiScale(face_region_gray, scaleFactor=1.05, minNeighbors=5)\n",
    "\n",
    "    #adjust OpenCV eye rectangles to be in the context of the whole image\n",
    "    opencv_eye_rects_global = []\n",
    "    for (ex, ey, ew, eh) in opencv_eye_rects:\n",
    "        opencv_eye_rects_global.append((x + ex, y + ey, ew, eh))\n",
    "\n",
    "    display_image_with_bounding_boxes(\n",
    "        image.copy(),\n",
    "        face_rect=(x, y, w, h),\n",
    "        left_eye_rect=left_eye_rect,\n",
    "        right_eye_rect=right_eye_rect,\n",
    "        opencv_eye_rects=opencv_eye_rects_global\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "#first column is image_id\n",
    "features = df.iloc[:, 1:].values  \n",
    "feature_names = df.columns[1:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "def mean(feature):\n",
    "    return sum(feature) / len(feature)\n",
    "\n",
    "# Function to calculate Pearson correlation between two features\n",
    "def pearson_correlation(feature_x, feature_y):\n",
    "    mean_x = mean(feature_x)\n",
    "    mean_y = mean(feature_y)\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator_x = 0\n",
    "    denominator_y = 0\n",
    "    \n",
    "    for i in range(len(feature_x)):\n",
    "        numerator += (feature_x[i] - mean_x) * (feature_y[i] - mean_y)\n",
    "        denominator_x += (feature_x[i] - mean_x) ** 2\n",
    "        denominator_y += (feature_y[i] - mean_y) ** 2\n",
    "    \n",
    "    denominator = math.sqrt(denominator_x) * math.sqrt(denominator_y)\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0  #avoid division by zero\n",
    "    return numerator / denominator\n",
    "\n",
    "num_features = features_scaled.shape[1]\n",
    "\n",
    "correlation_matrix = np.zeros((num_features, num_features))\n",
    "\n",
    "for i in range(num_features):\n",
    "    for j in range(i, num_features):\n",
    "        correlation_matrix[i][j] = pearson_correlation(features_scaled[:, i], features_scaled[:, j])\n",
    "        correlation_matrix[j][i] = correlation_matrix[i][j]\n",
    "\n",
    "# Function to select features based on correlation and variance\n",
    "def select_features(features, correlation_matrix, threshold=0.8):\n",
    "    selected_features = []\n",
    "\n",
    "    variances = np.var(features, axis=0)\n",
    "    \n",
    "    # Sort feature indices by variance in descending order\n",
    "    sorted_by_variance = np.argsort(-variances)\n",
    "    \n",
    "    for i in sorted_by_variance:\n",
    "        correlated = False\n",
    "        for j in selected_features:\n",
    "            if abs(correlation_matrix[i][j]) > threshold: \n",
    "                correlated = True\n",
    "                break\n",
    "        if not correlated:\n",
    "            selected_features.append(i)\n",
    "        if len(selected_features) == 6: \n",
    "            break\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "threshold = 0.8\n",
    "selected_feature_indices = select_features(features_scaled, correlation_matrix, threshold)\n",
    "\n",
    "selected_feature_names = feature_names[selected_feature_indices]\n",
    "\n",
    "print(f\"Selected Feature Names: {selected_feature_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "selected_columns = ['Wearing_Lipstick', 'Big_Nose', 'Blurry', 'Bangs', 'Chubby','Brown_Hair']\n",
    "# selected_columns = ['skin_color_score', 'eye_color_score', 'Blurry', 'Bangs', 'Chubby','Brown_Hair']\n",
    "X = df[selected_columns].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#Elbow method for KMeans\n",
    "def plot_elbow_method(X_scaled, max_k=10):\n",
    "    inertias = []\n",
    "    K = range(1, max_k+1)\n",
    "    \n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K, inertias, 'bo-', markersize=8)\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.xticks(K)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_elbow_method(X_scaled, max_k=10)\n",
    "\n",
    "#k-Distance plot for finding eps in DBSCAN\n",
    "def plot_k_distance(X_scaled, min_samples=10):\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "    neighbors_fit = neighbors.fit(X_scaled)\n",
    "    distances, _ = neighbors_fit.kneighbors(X_scaled)\n",
    "    \n",
    "    # Sort distances to find the \"elbow\" point\n",
    "    distances = np.sort(distances[:, min_samples-1], axis=0)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.title(f'k-Distance Plot for DBSCAN (k={min_samples})')\n",
    "    plt.xlabel('Data Points sorted by Distance to kth Neighbor')\n",
    "    plt.ylabel(f'{min_samples}-th Nearest Neighbor Distance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "noise = np.random.normal(0, 0.1, X_scaled.shape)\n",
    "X_noisy = X_scaled + noise\n",
    "plot_k_distance(X_noisy, min_samples=10)\n",
    "\n",
    "#Estimate initial bandwidth for MeanShift\n",
    "estimated_bandwidth = estimate_bandwidth(X_scaled, quantile=0.3)\n",
    "print(f\"Estimated Bandwidth for MeanShift: {estimated_bandwidth}\")\n",
    "\n",
    "def save_cluster_images(df, labels, num_clusters, image_column='image_id', save_dir='clusters'):\n",
    "    if os.path.exists(save_dir):\n",
    "        shutil.rmtree(save_dir) \n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "    for cluster_num in range(num_clusters):\n",
    "        cluster_folder = os.path.join(save_dir, f'cluster_{cluster_num}')\n",
    "        os.makedirs(cluster_folder)\n",
    "\n",
    "        cluster_images = df[df['cluster'] == cluster_num][image_column]\n",
    "\n",
    "        if len(cluster_images) <= 10:\n",
    "            images_to_save = cluster_images\n",
    "        else:\n",
    "            images_to_save = cluster_images.sample(10)\n",
    "\n",
    "        for image_name in images_to_save:\n",
    "            image_path = os.path.join('dataset', image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                shutil.copy(image_path, cluster_folder)\n",
    "            else:\n",
    "                print(f\"Image {image_name} not found\")\n",
    "\n",
    "\n",
    "def cluster_size_score(labels):\n",
    "    #Count the number of points in each cluster\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    #Check if noise is present and filter it out\n",
    "    if -1 in unique_labels:\n",
    "        noise_index = np.where(unique_labels == -1)[0][0]  #Find the index of the noise cluster\n",
    "        counts = np.delete(counts, noise_index)            #Remove noise count from counts array\n",
    "\n",
    "    #Calculate variance of cluster sizes as a measure of balance\n",
    "    score = np.var(counts)  #Lower variance means more balanced clusters\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# Hyperparameter tuning and clustering\n",
    "def run_clustering_and_evaluate(X_scaled, algorithm, params):\n",
    "    if algorithm == 'KMeans':\n",
    "        model = KMeans(n_clusters=params['n_clusters'], random_state=42)\n",
    "    elif algorithm == 'DBSCAN':\n",
    "        model = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "    elif algorithm == 'MeanShift':\n",
    "        model = MeanShift(bandwidth=params['bandwidth'])\n",
    "    \n",
    "    labels = model.fit_predict(X_scaled)\n",
    "    \n",
    "    score = cluster_size_score(labels)\n",
    "    print(f\"{algorithm} Cluster Size Score: {score}\")\n",
    "    return labels\n",
    "\n",
    "kmeans_params = {'n_clusters': 7}\n",
    "kmeans_labels = run_clustering_and_evaluate(X_scaled, 'KMeans', kmeans_params)\n",
    "# Save cluster images for kmeans\n",
    "df['cluster'] = kmeans_labels \n",
    "save_cluster_images(df, kmeans_labels, kmeans_params['n_clusters'], image_column='image_id', save_dir='clusters_kmeans')\n",
    "\n",
    "dbscan_params = {'eps': 0.6, 'min_samples': 10}  \n",
    "dbscan_labels = run_clustering_and_evaluate(X_scaled, 'DBSCAN', dbscan_params)\n",
    "\n",
    "num_dbscan_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)  # Ignore noise (-1) cluster\n",
    "\n",
    "# Save cluster images for DBSCAN\n",
    "df['cluster'] = dbscan_labels\n",
    "save_cluster_images(df, dbscan_labels, num_dbscan_clusters, image_column='image_id', save_dir='clusters_dbscan')\n",
    "\n",
    "meanshift_params = {'bandwidth': estimated_bandwidth}\n",
    "meanshift_labels = run_clustering_and_evaluate(X_scaled, 'MeanShift', meanshift_params)\n",
    "\n",
    "num_meanshift_clusters = len(set(meanshift_labels))\n",
    "\n",
    "# Save cluster images for MeanShift\n",
    "df['cluster'] = meanshift_labels  \n",
    "save_cluster_images(df, meanshift_labels, num_meanshift_clusters, image_column='image_id', save_dir='clusters_meanshift')\n",
    "\n",
    "# Plot heatmap of the features for each cluster\n",
    "def plot_heatmap(df, labels, selected_columns, name):\n",
    "    df['cluster'] = labels\n",
    "    cluster_means = df.groupby('cluster')[selected_columns].mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cluster_means, annot=True, cmap='coolwarm')\n",
    "    plt.title(name + \" Cluster Heatmap of Selected Features\")\n",
    "    plt.show()\n",
    "\n",
    "plot_heatmap(df, kmeans_labels, selected_columns, 'KMeans')\n",
    "plot_heatmap(df, dbscan_labels, selected_columns, 'DBSCAN')\n",
    "plot_heatmap(df, meanshift_labels, selected_columns, 'MeanShift')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "selected_columns = ['Wearing_Lipstick', 'Big_Nose', 'Blurry', 'Bangs', 'Chubby','Brown_Hair']\n",
    "X = df[selected_columns].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def run_clustering_and_evaluate(X_scaled, algorithm, params):\n",
    "    if algorithm == 'KMeans':\n",
    "        model = KMeans(n_clusters=params['n_clusters'], random_state=42)\n",
    "    elif algorithm == 'DBSCAN':\n",
    "        model = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "    elif algorithm == 'MeanShift':\n",
    "        model = MeanShift(bandwidth=params['bandwidth'])\n",
    "    \n",
    "    labels = model.fit_predict(X_scaled)\n",
    "    return labels\n",
    "\n",
    "kmeans_params = {'n_clusters': 7}\n",
    "kmeans_labels = run_clustering_and_evaluate(X_scaled, 'KMeans', kmeans_params)\n",
    "\n",
    "dbscan_params = {'eps': 0.6, 'min_samples': 10}\n",
    "dbscan_labels = run_clustering_and_evaluate(X_scaled, 'DBSCAN', dbscan_params)\n",
    "\n",
    "meanshift_params = {'bandwidth': 2.63}\n",
    "meanshift_labels = run_clustering_and_evaluate(X_scaled, 'MeanShift', meanshift_params)\n",
    "\n",
    "def reduce_dimensions(X, method='PCA'):\n",
    "    if method == 'PCA':\n",
    "        pca = PCA(n_components=2)\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "        print(f'Explained variance by PCA: {pca.explained_variance_ratio_}')\n",
    "    return X_reduced\n",
    "\n",
    "# Define colors for clusters and noise\n",
    "def get_custom_palette(labels):\n",
    "    unique_labels = set(labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    \n",
    "\n",
    "    palette = sns.color_palette(\"Set1\", n_colors=n_clusters)\n",
    "    palette = [(0.0, 0.0, 0.0)] + palette  #black for noise\n",
    "    return palette\n",
    "\n",
    "#function for visualizing clustering results\n",
    "def visualize_clusters(X_reduced, labels, title='Cluster Visualization'):\n",
    "    palette = get_custom_palette(labels)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=labels, palette=palette, s=50, alpha=0.7, edgecolor='k')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "X_reduced_pca = reduce_dimensions(X_scaled, method='PCA')\n",
    "\n",
    "visualize_clusters(X_reduced_pca, kmeans_labels, title='KMeans Clustering Visualization (PCA)')\n",
    "\n",
    "visualize_clusters(X_reduced_pca, dbscan_labels, title='DBSCAN Clustering Visualization (PCA)')\n",
    "\n",
    "visualize_clusters(X_reduced_pca, meanshift_labels, title='MeanShift Clustering Visualization (PCA)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features.csv')\n",
    "\n",
    "selected_columns = ['Wearing_Lipstick', 'Big_Nose', 'Blurry', 'Bangs', 'Chubby','Brown_Hair']\n",
    "X = df[selected_columns].values\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "#get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "df['kmeans_cluster'] = kmeans.labels_\n",
    "\n",
    "#use KNN to find the closest points to each cluster center\n",
    "def find_nearest_points(X, cluster_centers, num_points=50):\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=num_points)\n",
    "    nearest_neighbors.fit(X)\n",
    "    \n",
    "    #for each cluster center, find the nearest points\n",
    "    closest_points_indices = []\n",
    "    for center in cluster_centers:\n",
    "        distances, indices = nearest_neighbors.kneighbors([center])\n",
    "        closest_points_indices.append(indices.flatten())\n",
    "    return closest_points_indices\n",
    "\n",
    "#find 50 and 3000 closest points for each cluster center\n",
    "closest_50_indices = find_nearest_points(X, cluster_centers, num_points=50)\n",
    "closest_3000_indices = find_nearest_points(X, cluster_centers, num_points=3000)\n",
    "\n",
    "#analyze if the nearest points belong to the same cluster\n",
    "def analyze_cluster_membership(closest_indices, original_labels):\n",
    "    mismatches = []\n",
    "    for cluster_num, indices in enumerate(closest_indices):\n",
    "        cluster_label = cluster_num  #the label of the cluster center\n",
    "        mismatch_points = []\n",
    "        \n",
    "        for index in indices:\n",
    "            if original_labels[index] != cluster_label:\n",
    "                mismatch_points.append(index)\n",
    "        \n",
    "        mismatches.append(mismatch_points)\n",
    "    return mismatches\n",
    "\n",
    "def analyze_mismatch_details(mismatch_points, X, cluster_centers, original_labels):\n",
    "    categories = {'Overlapping': 0, 'Noise': 0, 'Misclassified': 0}\n",
    "    \n",
    "    for index in mismatch_points:\n",
    "        point = X[index]\n",
    "        distances = pairwise_distances([point], cluster_centers).flatten()  #distances to all centers\n",
    "        nearest_center = np.argmin(distances)  #closest center to the point\n",
    "        \n",
    "        #determine the mismatch category\n",
    "        if abs(distances[nearest_center] - np.partition(distances, 1)[1]) < 0.1:  #overlapping clusters\n",
    "            category = \"Overlapping\"\n",
    "        else:\n",
    "            category = \"Noise\" if distances[nearest_center] > np.mean(distances) else \"Misclassified\"\n",
    "        \n",
    "        categories[category] += 1\n",
    "    \n",
    "    return categories\n",
    "\n",
    "def report_mismatches(mismatches, num_points, X, cluster_centers, df):\n",
    "    for cluster_num, mismatch_points in enumerate(mismatches):\n",
    "        print(f\"Cluster {cluster_num}:\")\n",
    "        print(f\"Total mismatches for {num_points} points: {len(mismatch_points)}\")\n",
    "        \n",
    "        if mismatch_points:\n",
    "            categories = analyze_mismatch_details(mismatch_points, X, cluster_centers, df['kmeans_cluster'].values)\n",
    "            \n",
    "            print(f\"Overlapping: {categories['Overlapping']} points\")\n",
    "            print(f\"Noise: {categories['Noise']} points\")\n",
    "            print(f\"Misclassified: {categories['Misclassified']} points\")\n",
    "\n",
    "print(\"Analysis for 50 closest points to each cluster center:\")\n",
    "mismatches_50 = analyze_cluster_membership(closest_50_indices, df['kmeans_cluster'].values)\n",
    "report_mismatches(mismatches_50, 50, X, cluster_centers, df)\n",
    "\n",
    "print(\"\\nAnalysis for 3000 closest points to each cluster center:\")\n",
    "mismatches_3000 = analyze_cluster_membership(closest_3000_indices, df['kmeans_cluster'].values)\n",
    "report_mismatches(mismatches_3000, 3000, X, cluster_centers, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PHASE 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('features.csv')\n",
    "df_test = pd.read_csv('test_features.csv')  \n",
    "\n",
    "selected_columns = ['Wearing_Lipstick', 'Big_Nose', 'Blurry', 'Bangs', 'Chubby','Brown_Hair']\n",
    "X_train = df_train[selected_columns].values\n",
    "X_test = df_test[selected_columns].values\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "#Predict clusters for the test data\n",
    "df_test['predicted_cluster'] = kmeans.predict(X_test)\n",
    "\n",
    "df_test.to_csv('test_features.csv', index=False)\n",
    "\n",
    "save_dir = 'test_cluster_results'\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)  \n",
    "os.makedirs(save_dir)\n",
    "\n",
    "def save_images_for_test_data(df_test, X_train, df_train, kmeans, save_dir, image_column='image_id'):\n",
    "    sample_test_data = df_test.groupby('predicted_cluster').apply(lambda x: x.sample(1, random_state=42)).reset_index(drop=True)  # 1 test point per cluster\n",
    "\n",
    "    for idx, row in sample_test_data.iterrows():\n",
    "        test_point = row[selected_columns].values.reshape(1, -1)\n",
    "        predicted_cluster = row['predicted_cluster']\n",
    "        test_image_id = row[image_column]\n",
    "\n",
    "        test_image_folder = os.path.join(save_dir, f'test_image_{test_image_id}_cluster_{predicted_cluster}')\n",
    "        os.makedirs(test_image_folder, exist_ok=True)\n",
    "\n",
    "        test_image_path = os.path.join('test', test_image_id)\n",
    "        if os.path.exists(test_image_path):\n",
    "            shutil.copy(test_image_path, test_image_folder)\n",
    "\n",
    "        cluster_indices = np.where(kmeans.labels_ == predicted_cluster)[0]\n",
    "        cluster_sample_indices = np.random.choice(cluster_indices, 5, replace=False)\n",
    "        cluster_sample_data = df_train.iloc[cluster_sample_indices]\n",
    "\n",
    "        for cluster_idx, cluster_row in cluster_sample_data.iterrows():\n",
    "            cluster_image_id = cluster_row[image_column]\n",
    "            cluster_image_path = os.path.join('dataset', cluster_image_id) \n",
    "            if os.path.exists(cluster_image_path):\n",
    "                shutil.copy(cluster_image_path, test_image_folder)\n",
    "\n",
    "save_images_for_test_data(df_test, X_train, df_train, kmeans, save_dir)\n",
    "\n",
    "print(f\"Results saved in the folder: {save_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
